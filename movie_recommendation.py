# -*- coding: utf-8 -*-
"""movie recommendation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-MgAJQTkCE8P1KyaNy01j991naR9YjYM
"""

!apt-get update
# Download Java Virtual Machine (JVM)
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# Download Spark
!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
# Unzip the file
!tar xf spark-3.2.1-bin-hadoop3.2.tgz

!pip install -q findspark

# set your spark folder to your system path environment.
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = '/content/spark-3.2.1-bin-hadoop3.2'

!ls

import findspark
findspark.init()

findspark.find()

!pip install pyspark

import pyspark
from pyspark.sql import SparkSession
# Create a Spark Session
spark = SparkSession.builder.master("local[*]").getOrCreate()
# Check Spark Session Information
spark

from pyspark import SparkConf
from pyspark.context import SparkContext

sc = SparkContext.getOrCreate(SparkConf().setMaster("local[*]"))
sc

data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)

distData.collect()

distData.top(2)

distData.take(3)

distData.first()

distData.count()

distData.reduce(lambda x,y : x+y)

mappedData=distData.map(lambda x:x*x)

mappedData.collect()

m=sc.parallelize([('apple',1),('orange',5),('apple',6),('orange',3)]).keys()

m.collect()

m=sc.parallelize([('apple',1),('orange',5),('apple',6),('orange',3)]).values()

m.collect()

collection =['a', 'b', 'c','d','e']

rddfromcollection =sc.parallelize(collection)

rddfromcollection.collect()

rdd = sc.parallelize(["Hello VIT Hi VIT Welcome VIT"])
counts = rdd.flatMap(lambda line: line.split(" ")) \
    .map(lambda word: (word, 1)) \
    .reduceByKey(lambda a, b: a + b) \
    .collect()
print(counts)

sc

#Load data
!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip
!unzip ml-100k.zip

#Load data
user_data = sc.textFile("/content/order.txt.csv")

user_data.take(5)

#count number of customers
user_fields = user_data.map(lambda line: line.split("|"))
num_users = user_fields.map(lambda fields: fields[0]).count()

#count number of items
num_genders = user_fields.map(lambda fields:
fields[2]).distinct().count()

#count number of Qty
num_occupations = user_fields.map(lambda fields:
fields[3]).distinct().count()

#count number of Price
num_zipcodes = user_fields.map(lambda fields:
fields[4]).distinct().count()

#count number of Amount
num_zipcodes = user_fields.map(lambda fields:
fields[4]).distinct().count()

import matplotlib.pyplot as plt
import numpy as np

#a histogram to analyze the distribution of user items
ages = user_fields.map(lambda x: int(x[1])).collect()
plt.hist(ages, bins=20, color='lightblue')
plt.show()

#frequencies of the various occupations of our users
#mapreduce to count occurences of each occupation

count_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()
x_axis1 = np.array([c[0] for c in count_by_occupation])
y_axis1 = np.array([c[1] for c in count_by_occupation])

user_fields.take(5)

"""#Sort the count data
First create 2 numpy arrays - use argsort method of numpy select elements of each array
"""

x_axis = x_axis1[np.argsort(y_axis1)]
y_axis = y_axis1[np.argsort(y_axis1)]

import matplotlib
import matplotlib.pyplot as plt
pos = np.arange(len(x_axis))
width = 1.0
ax = plt.axes()
ax.set_xticks(pos + (width / 2))
ax.set_xticklabels(x_axis)
plt.bar(pos, y_axis, width, color='lightblue')
plt.xticks(rotation=30)
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(16, 10)

#count by occupation
count_by_occupation2 = user_fields.map(lambda fields: fields[3]).countByValue()
print ("Map-reduce approach:")
print (dict(count_by_occupation2))
print ("")
print ("countByValue approach:")
print (dict(count_by_occupitemation))

movie_data = sc.textFile("/content/order.txt.csv")
print(movie_data.take(5))
num_movies = movie_data.count()
print("Movies: ", num_movies)

def convert_year(x):
 try:
   return int(x[-4:])

 except:
   return 1900

movie_fields = movie_data.map(lambda lines: lines.split("|"))
years = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x))

years_filtered = years.filter(lambda x: x != 1900)

movie_ages = years_filtered.map(lambda yr: 1998-yr).countByValue()
values = movie_ages.values()
bins = movie_ages.keys()
type(bins)


plt.bar(values, list(bins), color='lightblue')
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(16,10)
plt.show()

print(rating_data_raw.first())
num_ratings = rating_data_raw.count()
print("Ratings:", num_ratings)

rating_data = rating_data_raw.map(lambda line: line.split("\t"))
ratings = rating_data.map(lambda fields: int(fields[2]))
max_rating = ratings.reduce(lambda x, y: max(x, y))
min_rating = ratings.reduce(lambda x, y: min(x, y))
mean_rating = ratings.reduce(lambda x, y: x + y) / num_ratings
median_rating = np.median(ratings.collect())
ratings_per_user = num_ratings / num_users
ratings_per_movie = num_ratings / num_movies
print("Min rating:", min_rating)
print("Max rating:", max_rating)
print("Average rating:", mean_rating)
print("Median rating:", median_rating)
print("Average # of ratings per user:", ratings_per_user)
print("Average # of ratings per movie:", ratings_per_movie)

v

ratings.stats()

from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating

# Load and parse the data
data = sc.textFile("/content/spark-3.2.1-bin-hadoop3.2/data/mllib/als/test.data")
ratings = data.map(lambda l: l.split(','))\
    .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))

# Build the recommendation model using Alternating Least Squares
rank = 10
numIterations = 10
model = ALS.trainImplicit(ratings, rank, numIterations, alpha=0.01)

# Evaluate the model on training data
testdata = ratings.map(lambda p: (p[0], p[1]))
predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))
ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)
MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()
print("Mean Squared Error = " + str(MSE))

# Save and load model
model.save(sc, "target/tmp/myCollaborativeFilter")
sameModel = MatrixFactorizationModel.load(sc, "target/tmp/myCollaborativeFilter")

data.collect()

ratings.collect()

testdata.collect()

predictions.collect()

ratesAndPreds.collect()

